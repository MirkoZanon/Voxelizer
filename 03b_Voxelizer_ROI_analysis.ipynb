{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT\n",
    "\n",
    "You should have **Voxelizer.py** in your pyhton working folder (download it here https://github.com/MirkoZanon/Voxelizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numan as nu\n",
    "import vodex as vx\n",
    "import Voxelizer as voxz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLDERS AND DATA\n",
    "\n",
    "The program assumes a main working folder inside which you should have a **'*mask.tif*' file** with a mask of ROIs defined by integers and a **'*/processed*' folder**.\n",
    "This 'processed' folder is previously created by numan (after Notebook 02) and should contain an additional **'*/drift_corrected_af_to1000*'** folder with the truncated drift corrected files and their relative annotation (number+shape+spread labels) saved as **'*stimuli_truncated_timelines.csv*'**.\n",
    "If these hierachies and names are not matching you have to modify them in the code below (or rename your original files)!\n",
    "\n",
    "The process will create (if not existing) a folder inside the main path called '*/voxelizer_final_datasets*', where it will output a series of csv files for each ROI of the mask (file output name code: fishInfo_ROI#.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you declared that your dataset is cut in \u001b[1mchunks of 9 volumes\u001b[0m. This info should be fulfilled by data in /drift_corrected_af_to1000 for a correct analysis!\n"
     ]
    }
   ],
   "source": [
    "# raw dataset folder and info\n",
    "base_folder = \"/Users/mirckuz/Desktop/20230601_Hz09_casper_h2bcamp7f_7dpf_60Z_1hzvol_2P_1v2v3v4v5_processed\"\n",
    "fishInfo = 'Hz09_casper_h2bcamp7f_7dpf'\n",
    "analyze_SV_or_Imaris = 0 # 0 for Super Voxel analysis, 1 for Imaris spot analysis\n",
    "\n",
    "# parameters for analysis\n",
    "superVoxel_size = [5,5,5] # dimention of segmentation 'super voxel' in voxels. IMPORTANT, coordinates order: [z,y,x]\n",
    "ROIs_to_analyze = [2,3,4,5] # define the ROIs you want to analyze (the ROI ID is the integer number relative to your mask)\n",
    "truncated_stimulus_chunck_dimention = 9 # number of volumes in truncated window (remember python starts counting from zero)\n",
    "normalization_frames = [0,1,2] # frames in truncated window to consider as F0 background for dF/F0 normalization\n",
    "frames_per_volume = 60 # orginal frame per volume in raw numan experiment\n",
    "starting_slice = 0 # orginal starting slice in raw numan experiment\n",
    "n_vol_batch = 8 # number of volumes to load at a time for batch analysis\n",
    "\n",
    "print('Note: you declared that your dataset is cut in \\033[1mchunks of '+str(truncated_stimulus_chunck_dimention)+' volumes\\033[0m. This info should be fulfilled by data in /drift_corrected_af_to1000 for a correct analysis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment...\n",
      "Time volumes: 2160. Divisible by 9: True\n"
     ]
    }
   ],
   "source": [
    "os.chdir(base_folder) # set working directory to main raw data folder defined above\n",
    "mask_file = \"mask.tif\"\n",
    "annotations = pd.read_csv('./processed/stimuli_truncated_timelines.csv')\n",
    "print('Loading experiment...')\n",
    "experiment = vx.Experiment.from_dir('./processed/drift_corrected_af_to1000',frames_per_volume,starting_slice,verbose=False)\n",
    "experiment.add_annotations_from_volume_annotation_df(annotations)\n",
    "print('Time volumes: '+str(experiment.n_volumes)+'. Divisible by '+ str(truncated_stimulus_chunck_dimention)+': '+str(experiment.n_volumes%truncated_stimulus_chunck_dimention==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_SV_or_Imaris == 1:\n",
    "\n",
    "    my_spots = nu.Spots.from_json(\"./processed/spots/signals/spots_SvB_max.json\")\n",
    "    print('Normalizing signal...')\n",
    "    dff = my_spots.signals.as_dff(method=\"step\", step_size=truncated_stimulus_chunck_dimention, baseline_volumes=normalization_frames)\n",
    "    signals = dff.traces.T\n",
    "\n",
    "    stim_volumes = experiment.choose_volumes([(\"number\",\"d1\"), (\"number\",\"d2\"), (\"number\",\"d3\"),(\"number\",\"d4\"), (\"number\",\"d5\")], logic = \"or\")\n",
    "    stim_signal_exact = signals[:,stim_volumes]\n",
    "    stim_signal_prov = np.zeros((stim_signal_exact.shape[0],stim_signal_exact.shape[1],3))\n",
    "    stim_signal_prov[:,:,0] = stim_signal_exact\n",
    "    for i in [1,2]: # add additional volume after stimulus to calculate final avg signal\n",
    "        stim_add_volumes = [x+i for x in stim_volumes]\n",
    "        stim_add_signal = signals[:,stim_add_volumes]\n",
    "        stim_signal_prov[:,:,i] = stim_add_signal\n",
    "    stim_signal = stim_signal_prov.mean(axis=2)\n",
    "    print(' You obtained -> SuperVoxels X trials: ' + str(stim_signal.shape)+'\\n')\n",
    "        \n",
    "    annotation_dict2= {f\"cell_{ic}\": stim_signal[ic] for ic in np.arange(len(signals))}\n",
    "    annotation_dict=experiment.get_volume_annotations(stim_volumes)\n",
    "    annotation_dict.update(annotation_dict2)\n",
    "    annotation_df=pd.DataFrame(annotation_dict)\n",
    "\n",
    "    #calculate control trials label array\n",
    "    C_pd = pd.factorize((annotation_df['shape']+ annotation_df['spread']), sort=True)\n",
    "    #C_pd = pd.factorize((annotation_df['shape']), sort=True)\n",
    "    labelC = C_pd[1]\n",
    "    C = np.array(C_pd[0])\n",
    "    print('Control conditions label array, shape -> (trials,): ' + str(C.shape))\n",
    "\n",
    "    #calculate stimulus trials label array\n",
    "    Q_pd = pd.factorize(annotation_df['number'], sort=True)\n",
    "    labelQ = Q_pd[1]\n",
    "    Q = np.array(Q_pd[0])\n",
    "    print('Stimulus label array, shape -> (trials,): ' + str(Q.shape))\n",
    "\n",
    "    Hf = np.array(annotation_df.iloc[:, 4:annotation_df.shape[1]])\n",
    "    print('Final dataset, shape -> (trials,cells): ' + str(Hf.shape))\n",
    "\n",
    "    # merge the three array of interest Hf(data), Q(condition), C(control)\n",
    "    final_labels = np.stack((C,Q),axis=1)\n",
    "    total_df = np.concatenate((final_labels, Hf),axis=1)\n",
    "\n",
    "    # save at /imaris_final_datasets\n",
    "    if not os.path.exists('./imaris_final_datasets'):\n",
    "        os.makedirs('./imaris_final_datasets')\n",
    "    np.savetxt('./imaris_final_datasets/'+fishInfo+'.csv', total_df, delimiter=',')\n",
    "    print('!FINAL ARRAY SAVED AS CSV!, shape -> (trials,2+cells): ' + str(total_df.shape))\n",
    "\n",
    "    print('\\033[1m\\nDone! Your new raw dataset for Imaris based analysis is saved as csv file.\\033[0m\\nYou can find it in ./imaris_final_datasets folder.\\nIf you want to anlayze it and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "PROCESSING ROI 2\u001b[0m\n",
      "\n",
      "Voxelizing ROI 2... This could take time depending on your data and ROI size, but approx less than 20 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46ad7f9646d4e6487343074a27abc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voxelizing chunks:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing signal in ROI 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5525141a79d4c61a57bb270a797fe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization cycles:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data table for ROI 2...\n",
      " You obtained -> SuperVoxels X trials: (30, 240)\n",
      "\n",
      "Control conditions label array, shape -> (trials,): (240,)\n",
      "Stimulus label array, shape -> (trials,): (240,)\n",
      "Final dataset ROI 2, shape -> (trials,cells): (240, 30)\n",
      "!FINAL ARRAY FOR ROI 2 SAVED AS CSV!, shape -> (trials,2+cells): (240, 32)\n",
      "\u001b[1m\n",
      "Done! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you selected.\u001b[0m\n",
      "You can find them in ./voxelizer_final_datasets.\n",
      "If you want to anlayze them and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder\n",
      "\u001b[1m\n",
      "PROCESSING ROI 3\u001b[0m\n",
      "\n",
      "Voxelizing ROI 3... This could take time depending on your data and ROI size, but approx less than 20 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af3ed231f764bbf9ac66860d2890de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voxelizing chunks:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing signal in ROI 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39b2acc9ee5411fa6f335c94f1cf5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization cycles:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data table for ROI 3...\n",
      " You obtained -> SuperVoxels X trials: (75, 240)\n",
      "\n",
      "Control conditions label array, shape -> (trials,): (240,)\n",
      "Stimulus label array, shape -> (trials,): (240,)\n",
      "Final dataset ROI 3, shape -> (trials,cells): (240, 75)\n",
      "!FINAL ARRAY FOR ROI 3 SAVED AS CSV!, shape -> (trials,2+cells): (240, 77)\n",
      "\u001b[1m\n",
      "Done! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you selected.\u001b[0m\n",
      "You can find them in ./voxelizer_final_datasets.\n",
      "If you want to anlayze them and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder\n",
      "\u001b[1m\n",
      "PROCESSING ROI 4\u001b[0m\n",
      "\n",
      "Voxelizing ROI 4... This could take time depending on your data and ROI size, but approx less than 20 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aed68bc9f3466db8755f4d1509c3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voxelizing chunks:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing signal in ROI 4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4a9452bba4962bc286d077e839cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization cycles:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data table for ROI 4...\n",
      " You obtained -> SuperVoxels X trials: (48, 240)\n",
      "\n",
      "Control conditions label array, shape -> (trials,): (240,)\n",
      "Stimulus label array, shape -> (trials,): (240,)\n",
      "Final dataset ROI 4, shape -> (trials,cells): (240, 48)\n",
      "!FINAL ARRAY FOR ROI 4 SAVED AS CSV!, shape -> (trials,2+cells): (240, 50)\n",
      "\u001b[1m\n",
      "Done! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you selected.\u001b[0m\n",
      "You can find them in ./voxelizer_final_datasets.\n",
      "If you want to anlayze them and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder\n",
      "\u001b[1m\n",
      "PROCESSING ROI 5\u001b[0m\n",
      "\n",
      "Voxelizing ROI 5... This could take time depending on your data and ROI size, but approx less than 20 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca0bba6b463410dac2b5acce1f629ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voxelizing chunks:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing signal in ROI 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d918978ed8c4d0f93514a78b4e61082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalization cycles:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data table for ROI 5...\n",
      " You obtained -> SuperVoxels X trials: (480, 240)\n",
      "\n",
      "Control conditions label array, shape -> (trials,): (240,)\n",
      "Stimulus label array, shape -> (trials,): (240,)\n",
      "Final dataset ROI 5, shape -> (trials,cells): (240, 480)\n",
      "!FINAL ARRAY FOR ROI 5 SAVED AS CSV!, shape -> (trials,2+cells): (240, 482)\n",
      "\u001b[1m\n",
      "Done! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you selected.\u001b[0m\n",
      "You can find them in ./voxelizer_final_datasets.\n",
      "If you want to anlayze them and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder\n"
     ]
    }
   ],
   "source": [
    "if analyze_SV_or_Imaris == 0:\n",
    "    for roi in ROIs_to_analyze:\n",
    "        print('\\033[1m\\nPROCESSING ROI '+str(roi)+'\\033[0m\\n')\n",
    "\n",
    "        print('Voxelizing ROI '+str(roi)+'... This could take time depending on your data and ROI size, but approx less than 20 min')\n",
    "        voxelizer = voxz.Voxelizer(mask_file, superVoxel_size, roi)\n",
    "        table = voxelizer.process_movie(experiment, n_vol_batch)\n",
    "\n",
    "        print('Normalizing signal in ROI '+str(roi)+'...')\n",
    "        df = voxelizer.create_normalized_signal_df(table,truncated_stimulus_chunck_dimention,normalization_frames)\n",
    "        signals = np.array(df).T\n",
    "        \n",
    "        print('Preparing data table for ROI '+str(roi)+'...')\n",
    "\n",
    "        stim_volumes = experiment.choose_volumes([(\"number\",\"d1\"), (\"number\",\"d2\"), (\"number\",\"d3\"),(\"number\",\"d4\"), (\"number\",\"d5\")], logic = \"or\")\n",
    "        stim_signal_exact = signals[:,stim_volumes]\n",
    "        stim_signal_prov = np.zeros((stim_signal_exact.shape[0],stim_signal_exact.shape[1],3))\n",
    "        stim_signal_prov[:,:,0] = stim_signal_exact\n",
    "        for i in [1,2]: # add additional volume after stimulus to calculate final avg signal\n",
    "            stim_add_volumes = [x+i for x in stim_volumes]\n",
    "            stim_add_signal = signals[:,stim_add_volumes]\n",
    "            stim_signal_prov[:,:,i] = stim_add_signal\n",
    "        stim_signal = stim_signal_prov.mean(axis=2)\n",
    "        print(' You obtained -> SuperVoxels X trials: ' + str(stim_signal.shape)+'\\n')\n",
    "        \n",
    "        annotation_dict2= {f\"cell_{ic}\": stim_signal[ic] for ic in np.arange(len(signals))}\n",
    "        annotation_dict=experiment.get_volume_annotations(stim_volumes)\n",
    "        annotation_dict.update(annotation_dict2)\n",
    "        annotation_df=pd.DataFrame(annotation_dict)\n",
    "\n",
    "        #calculate control trials label array\n",
    "        C_pd = pd.factorize((annotation_df['shape']+ annotation_df['spread']), sort=True)\n",
    "        #C_pd = pd.factorize((annotation_df['shape']), sort=True)\n",
    "        labelC = C_pd[1]\n",
    "        C = np.array(C_pd[0])\n",
    "        print('Control conditions label array, shape -> (trials,): ' + str(C.shape))\n",
    "\n",
    "        #calculate stimulus trials label array\n",
    "        Q_pd = pd.factorize(annotation_df['number'], sort=True)\n",
    "        labelQ = Q_pd[1]\n",
    "        Q = np.array(Q_pd[0])\n",
    "        print('Stimulus label array, shape -> (trials,): ' + str(Q.shape))\n",
    "\n",
    "        Hf = np.array(annotation_df.iloc[:, 4:annotation_df.shape[1]])\n",
    "        print('Final dataset ROI '+str(roi)+', shape -> (trials,cells): ' + str(Hf.shape))\n",
    "\n",
    "        # merge the three array of interest Hf(data), Q(condition), C(control)\n",
    "        final_labels = np.stack((C,Q),axis=1)\n",
    "        total_df = np.concatenate((final_labels, Hf),axis=1)\n",
    "\n",
    "        # save at /voxelizer_final_datasets\n",
    "        if not os.path.exists('./voxelizer_final_datasets'):\n",
    "            os.makedirs('./voxelizer_final_datasets')\n",
    "        np.savetxt('./voxelizer_final_datasets/'+fishInfo+'_SV'+str(superVoxel_size[2])+'x'+str(superVoxel_size[1])+'x'+str(superVoxel_size[0])+'_ROI'+str(roi)+'.csv', total_df, delimiter=',')\n",
    "        print('!FINAL ARRAY FOR ROI '+str(roi)+' SAVED AS CSV!, shape -> (trials,2+cells): ' + str(total_df.shape))\n",
    "\n",
    "        print('\\033[1m\\nDone! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you selected.\\033[0m\\nYou can find them in ./voxelizer_final_datasets.\\nIf you want to anlayze them and check tunings run notebook 03c_Voxelizer_ANOVA_Nieder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final csv is a table with super voxels as rows and stimulus time volumes as columns, with values corresponding to the df signal for each stimulus (avg in a window of 3 volumes from stimulus). This is what we need for classical ANOVA analysis. (The whole signal for all time volumes is instead inside variable *signals*).\n",
    "Thus, final csv dimentions: SVs x stimulations "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voxelizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
