{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have Voxelizer.py in your working folder, and inside your data 'processed' folder from numan you should have the 'drift_corrected_af_to1000' folder with drift corrected truncated dataset from numan notebook #02\n",
    "\n",
    "You have to run (and save) this multiple times for multiple ROIs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numan as nu\n",
    "import vodex as vx\n",
    "import Voxelizer as voxz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import models, analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define folders here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw dataset folder (even a list of multiple fish could be ok, but you loose their identity)\n",
    "base_folder = \"/Users/mirckuz/Desktop/\"\n",
    "folders = [base_folder + \"20230601_Hz09_casper_h2bcamp7f_7dpf_60Z_1hzvol_2P_1v2v3v4v5_processed\"]\n",
    "\n",
    "# .tif mask of your brain\n",
    "mask_file = \"/Users/mirckuz/Desktop/Voxelizer/mask.tif\"\n",
    "# truncated annotation (numerosity, shape, spread) compatible with drift corrected data in 'drift_corrected_af_to1000' from numan notebook #02\n",
    "annotation4 = \"/Users/mirckuz/Desktop/Voxelizer/stimuli_truncated_timelines.csv\" \n",
    "\n",
    "# analyzed data saving - remeber to change name accordingly!!\n",
    "save_folder = '/Users/mirckuz/Desktop/'\n",
    "file_name = 'fishx_dayx_roix.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters and ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superVoxel_size = [3,3,3] # order: z,y,x\n",
    "roi_id = 1 #select mask integer\n",
    "\n",
    "frames_per_volume = 60\n",
    "starting_slice = 0\n",
    "n_vol_batch = 8\n",
    "analyze_SV_or_Imaris = 0 #0 = SV, 1 = Imaris Spots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run super voxel analysis analysis - it takes some time depending on recording and ROI size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_reg = pd.read_csv(annotation4)\n",
    "\n",
    "for count,f in enumerate(folders):\n",
    "    print('Processing: '+f)\n",
    "    os.chdir(f+'/processed/')\n",
    "    #os.chdir(f)\n",
    "    data_dir = 'drift_corrected_af_to1000'\n",
    "    print('Loading experiment...')\n",
    "    experiment = vx.Experiment.from_dir(data_dir,frames_per_volume,starting_slice,verbose=False)\n",
    "    #experiment.add_annotations_from_df(annotation_df1, cycles=True)\n",
    "    #experiment.add_annotations_from_df(annotation_df2, cycles=True)\n",
    "    #experiment.add_annotations_from_df(annotation_df3, cycles=True)\n",
    "    experiment.add_annotations_from_volume_annotation_df(annotation_reg)\n",
    "    experiment.labels_df\n",
    "\n",
    "    if analyze_SV_or_Imaris == 1:\n",
    "        my_spots = nu.Spots.from_json(\"spots/signals/spots_SvB_max.json\")\n",
    "        dff = my_spots.signals.as_dff(15)\n",
    "        signals = dff.traces.T\n",
    "    elif analyze_SV_or_Imaris == 0:\n",
    "        print('Voxelizing...')\n",
    "        voxelizer = voxz.Voxelizer(mask_file, superVoxel_size, roi_id)\n",
    "        table = voxelizer.process_movie(experiment, n_vol_batch)\n",
    "        print('Normalizing signal...')\n",
    "        #df = voxelizer.create_signal_df(table)\n",
    "        df = voxelizer.create_normalized_signal_df(table,3,[0,1,2])\n",
    "        signals = np.array(df).T\n",
    "    #print(signals)\n",
    "    print(signals.shape)\n",
    "    \n",
    "    print('Preparing data table...')\n",
    "    stim_volumes = experiment.choose_volumes([(\"number\",\"d1\"), (\"number\",\"d2\"), (\"number\",\"d3\"),(\"number\",\"d4\"), (\"number\",\"d5\")], logic = \"or\")\n",
    "    stim_signal1=signals[:,stim_volumes]\n",
    "    stim_signal2=signals[:,[x + 1 for x in stim_volumes]]\n",
    "    stim_signal3=signals[:,[x + 2 for x in stim_volumes]]\n",
    "    stim_signal4=signals[:,[x + 3 for x in stim_volumes]]\n",
    "    stim_signal = (stim_signal1+stim_signal2+stim_signal3+stim_signal4)/4\n",
    "    stim_signal.shape\n",
    "    print('cells X trials: ' + str(stim_signal.shape)+'\\n')\n",
    "    \n",
    "    annotation_dict2= {f\"cell_{ic}\": stim_signal[ic] for ic in np.arange(len(signals))}\n",
    "    annotation_dict=experiment.get_volume_annotations(stim_volumes)\n",
    "    annotation_dict.update(annotation_dict2)\n",
    "    annotation_df=pd.DataFrame(annotation_dict)\n",
    "    #print(annotation_df.shape)\n",
    "    \n",
    "    Hf = np.array(annotation_df.iloc[:, 4:annotation_df.shape[1]])\n",
    "    if count==0:\n",
    "        final_Hf = Hf\n",
    "    else:\n",
    "        final_Hf = np.append(final_Hf, Hf, 1)\n",
    "print('Total trials X cells: ' + str(final_Hf.shape))\n",
    "\n",
    "Q_pd = pd.factorize(annotation_df['number'], sort=True)\n",
    "labelQ = Q_pd[1]\n",
    "print(labelQ)\n",
    "Q = np.array(Q_pd[0])\n",
    "#print(Q)\n",
    "print(Q.shape)\n",
    "\n",
    "C_pd = pd.factorize((annotation_df['shape']+ annotation_df['spread']), sort=True)\n",
    "#C_pd = pd.factorize((annotation_df['shape']), sort=True)\n",
    "labelC = C_pd[1]\n",
    "print(labelC)\n",
    "C = np.array(C_pd[0])\n",
    "#print(C)\n",
    "print(C.shape)\n",
    "print('Done! Now go on and save overall data matrix with all SVs or continue with ANOVA analysis!')\n",
    "#/Volumes/Group Share/Neuroanalysis/Numerosity/1v2v3v4v5_H2B/20230207_hb02_casper_h2bcamp7f_5dpf_2P_1v2v3v4v5_1_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save final data as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = np.stack((C,Q),axis=1)\n",
    "total_df = np.concatenate((final_labels, final_Hf),axis=1)\n",
    "total_df.shape\n",
    "\n",
    "np.savetxt(save_folder+file_name, total_df, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voxelizer",
   "language": "python",
   "name": "voxelizer"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
