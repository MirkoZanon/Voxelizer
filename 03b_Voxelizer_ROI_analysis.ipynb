{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT\n",
    "\n",
    "You should have **Voxelizer.py** in your pyhton working folder (download it here https://github.com/MirkoZanon/Voxelizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numan as nu\n",
    "import vodex as vx\n",
    "import Voxelizer as voxz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOLDERS AND DATA\n",
    "\n",
    "The program assumes a main working folder (to set here, the program will then work inside this main folder) \n",
    "inside which you should have a **'*mask.tif*' file** with a mask of ROIs defined by integers and a **'*/processed*' folder**.\n",
    "This 'processed' folder is previously created by numan and should contain an additional **'*/drift_corrected_af_to1000*'** folder with the truncated drift corrected files and their relative annotation (number+shape+spread labels) saved as **'*stimuli_truncated_timelines.csv*'**.\n",
    "If these hierachies and names are not matching you have to modify them in the code below (or rename your original files)!\n",
    "\n",
    "The process will create (if not existing) a folder inside the main path called '*/voxelizer_final_datasets*', where it will output a series of csv files for each ROI of the mask (file output name code: fishInfo_ROI#.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you declared that your dataset is cut in \u001b[1mchunks of 9 volumes\u001b[0m. This info should be fulfilled by data in /drift_corrected_af_to1000 for a correct analysis!\n"
     ]
    }
   ],
   "source": [
    "# raw dataset folder and info\n",
    "base_folder = \"/Users/mirckuz/Desktop/20230601_Hz09_casper_h2bcamp7f_7dpf_60Z_1hzvol_2P_1v2v3v4v5_processed\"\n",
    "fishInfo = 'Hz09_casper_h2bcamp7f_7dpf'\n",
    "\n",
    "# parameters for analysis\n",
    "superVoxel_size = [3,3,3] # dimention of segmentation 'super voxel' in voxels. IMPORTANT, coordinates order: [z,y,x]\n",
    "ROIs_to_analyze = [1,2,3,4,5] # define the ROIs you want to analyze (the ROI ID is the integer number relative to your mask)\n",
    "truncated_stimulus_chunck_dimention = 9 # number of volumes in truncated window (remember python starts counting from zero)\n",
    "normalization_frames = [0,1,2] # frames in truncated window to consider as F0 background for dF/F0 normalization\n",
    "frames_per_volume = 60 # orginal frame per volume in raw numan experiment\n",
    "starting_slice = 0 # orginal starting slice in raw numan experiment\n",
    "n_vol_batch = 8 # number of volumes to load at a time for batch analysis\n",
    "\n",
    "print('Note: you declared that your dataset is cut in \\033[1mchunks of '+str(truncated_stimulus_chunck_dimention)+' volumes\\033[0m. This info should be fulfilled by data in /drift_corrected_af_to1000 for a correct analysis!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to go! Run super voxel analysis analysis - this takes some time depending on recording and ROI size! (Should be approx 30min for an avareage ROI size and standard truncated exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiment...\n",
      "\u001b[1m\n",
      "PROCESSING ROI 1\u001b[0m\n",
      "\n",
      "Voxelizing ROI 1... This could take time depending on your data and ROI size, but approx less than 20 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b916a4699e494eeca07c52131a8bbe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Voxelizing chunks:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir(base_folder) # set working directory to main raw data folder defined above\n",
    "mask_file = \"mask.tif\"\n",
    "annotations = pd.read_csv('./processed/stimuli_truncated_timelines.csv')\n",
    "print('Loading experiment...')\n",
    "experiment = vx.Experiment.from_dir('./processed/drift_corrected_af_to1000',frames_per_volume,starting_slice,verbose=False)\n",
    "experiment.add_annotations_from_volume_annotation_df(annotations)\n",
    "\n",
    "for roi in ROIs_to_analyze:\n",
    "    print('\\033[1m\\nPROCESSING ROI '+str(roi)+'\\033[0m\\n')\n",
    "\n",
    "    print('Voxelizing ROI '+str(roi)+'... This could take time depending on your data and ROI size, but approx less than 20 min')\n",
    "    voxelizer = voxz.Voxelizer(mask_file, superVoxel_size, roi)\n",
    "    table = voxelizer.process_movie(experiment, n_vol_batch)\n",
    "\n",
    "    print('Normalizing signal in ROI '+str(roi)+'...')\n",
    "    df = voxelizer.create_normalized_signal_df(table,truncated_stimulus_chunck_dimention,normalization_frames)\n",
    "    signals = np.array(df).T\n",
    "    \n",
    "    print('Preparing data table for ROI '+str(roi)+'...')\n",
    "    stim_volumes = experiment.choose_volumes([(\"number\",\"d1\"), (\"number\",\"d2\"), (\"number\",\"d3\"),(\"number\",\"d4\"), (\"number\",\"d5\")], logic = \"or\")\n",
    "    stim_signal1=signals[:,stim_volumes]\n",
    "    stim_signal2=signals[:,[x + 1 for x in stim_volumes]]\n",
    "    stim_signal3=signals[:,[x + 2 for x in stim_volumes]]\n",
    "    stim_signal = (stim_signal1+stim_signal2+stim_signal3)/3\n",
    "    print(' You obtained -> SuperVoxels X trials: ' + str(stim_signal.shape)+'\\n')\n",
    "    \n",
    "    annotation_dict2= {f\"cell_{ic}\": stim_signal[ic] for ic in np.arange(len(signals))}\n",
    "    annotation_dict=experiment.get_volume_annotations(stim_volumes)\n",
    "    annotation_dict.update(annotation_dict2)\n",
    "    annotation_df=pd.DataFrame(annotation_dict)\n",
    "\n",
    "    #calculate control trials label array\n",
    "    C_pd = pd.factorize((annotation_df['shape']+ annotation_df['spread']), sort=True)\n",
    "    #C_pd = pd.factorize((annotation_df['shape']), sort=True)\n",
    "    labelC = C_pd[1]\n",
    "    C = np.array(C_pd[0])\n",
    "    print('Control conditions label array, shape -> (trials,): ' + str(C.shape))\n",
    "\n",
    "    #calculate stimulus trials label array\n",
    "    Q_pd = pd.factorize(annotation_df['number'], sort=True)\n",
    "    labelQ = Q_pd[1]\n",
    "    Q = np.array(Q_pd[0])\n",
    "    print('Stimulus label array, shape -> (trials,): ' + str(Q.shape))\n",
    "\n",
    "    Hf = np.array(annotation_df.iloc[:, 4:annotation_df.shape[1]])\n",
    "    print('Final dataset ROI '+str(roi)+', shape -> (trials,cells): ' + str(Hf.shape))\n",
    "\n",
    "    # merge the three array of interest Hf(data), Q(condition), C(control)\n",
    "    final_labels = np.stack((C,Q),axis=1)\n",
    "    total_df = np.concatenate((final_labels, Hf),axis=1)\n",
    "\n",
    "    # save at /voxelizer_final_datasets\n",
    "    if not os.path.exists('./voxelizer_final_datasets'):\n",
    "        os.makedirs('./voxelizer_final_datasets')\n",
    "    np.savetxt('./voxelizer_final_datasets/'+fishInfo+'_ROI'+str(roi)+'.csv', total_df, delimiter=',')\n",
    "    print('!FINAL ARRAY FOR ROI '+str(roi)+' SAVED AS CSV!, shape -> (trials,2+cells): ' + str(total_df.shape))\n",
    "\n",
    "print('\\033[1m\\nDone! Your new raw datasets for Super Voxel based analysis are saved as separated csv files for each one of the ROIs you sleected.\\033[0mYou can find them in ./voxelizer_final_datasets folder. If you want to anlayze them and check tuning run notebook 03c_Voxelizer_ANOVA_Nieder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voxelizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
